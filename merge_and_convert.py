"""
Script pour fusionner le LoRA avec le modÃ¨le de base et convertir en GGUF
Ã€ exÃ©cuter sur la VM aprÃ¨s l'entraÃ®nement
"""
from unsloth import FastLanguageModel
import torch

print("ğŸš€ Fusion du LoRA avec le modÃ¨le de base...")

# Charger le modÃ¨le avec l'adapter LoRA
print("ğŸ“¥ Chargement du modÃ¨le avec l'adapter LoRA...")
model, tokenizer = FastLanguageModel.from_pretrained(
    model_name="./blablabruti-lora-final",
    max_seq_length=2048,
    dtype=None,
    load_in_4bit=True,
)

# Fusionner le LoRA avec le modÃ¨le de base et sauvegarder
print("ğŸ”— Fusion du LoRA et sauvegarde...")
# Utiliser la mÃ©thode Unsloth qui fusionne et sauvegarde en une seule Ã©tape
# Si save_pretrained_merged ne fonctionne pas, essayez avec merge_and_unload d'abord
try:
    # MÃ©thode 1 : Utiliser save_pretrained_merged (recommandÃ©)
    model.save_pretrained_merged(
        "blablabruti-merged",
        tokenizer,
        save_method="merged_16bit",  # Sauvegarde en 16-bit pour Ã©conomiser l'espace
    )
except AttributeError:
    # MÃ©thode 2 : Fusionner d'abord, puis sauvegarder
    print("âš ï¸  Utilisation de la mÃ©thode alternative...")
    from peft import PeftModel
    if isinstance(model, PeftModel):
        model = model.merge_and_unload()
    model.save_pretrained("blablabruti-merged")
    tokenizer.save_pretrained("blablabruti-merged")

print("âœ… ModÃ¨le fusionnÃ© sauvegardÃ© dans ./blablabruti-merged")
print("\nğŸ“ Prochaines Ã©tapes :")
print("1. Le modÃ¨le fusionnÃ© est dans ./blablabruti-merged")
print("2. Vous pouvez maintenant convertir en GGUF avec llama.cpp")
print("3. Ou utiliser directement avec transformers/Unsloth")

