"""
Script pour fusionner le LoRA avec le modÃ¨le de base et convertir en GGUF
Ã€ exÃ©cuter sur la VM aprÃ¨s l'entraÃ®nement
"""
from unsloth import FastLanguageModel
from unsloth.is_pytorch_2_0_plus import is_pytorch_2_0_plus
import torch

print("ğŸš€ Fusion du LoRA avec le modÃ¨le de base...")

# Charger le modÃ¨le de base
print("ğŸ“¥ Chargement du modÃ¨le de base...")
base_model, tokenizer = FastLanguageModel.from_pretrained(
    model_name="unsloth/mistral-7b-v0.3-bnb-4bit",
    max_seq_length=2048,
    dtype=None,
    load_in_4bit=True,
)

# Charger l'adapter LoRA
print("ğŸ“¥ Chargement de l'adapter LoRA...")
model = FastLanguageModel.from_pretrained(
    model_name="./blablabruti-lora-final",
    max_seq_length=2048,
    dtype=None,
    load_in_4bit=True,
)

# Fusionner le LoRA avec le modÃ¨le de base
print("ğŸ”— Fusion du LoRA...")
model = FastLanguageModel.merge_and_unload(model)

# Sauvegarder le modÃ¨le fusionnÃ©
print("ğŸ’¾ Sauvegarde du modÃ¨le fusionnÃ©...")
model.save_pretrained_merged(
    "blablabruti-merged",
    tokenizer,
    save_method="merged_16bit",  # Sauvegarde en 16-bit pour Ã©conomiser l'espace
)
tokenizer.save_pretrained("blablabruti-merged")

print("âœ… ModÃ¨le fusionnÃ© sauvegardÃ© dans ./blablabruti-merged")
print("\nğŸ“ Prochaines Ã©tapes :")
print("1. Le modÃ¨le fusionnÃ© est dans ./blablabruti-merged")
print("2. Vous pouvez maintenant convertir en GGUF avec llama.cpp")
print("3. Ou utiliser directement avec transformers/Unsloth")

