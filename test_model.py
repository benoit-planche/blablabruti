from unsloth import FastLanguageModel
from transformers import TextStreamer

print("üöÄ Chargement du mod√®le entra√Æn√©...")

# Charger le mod√®le entra√Æn√©
model, tokenizer = FastLanguageModel.from_pretrained(
    model_name="./blablabruti-lora-final",
    max_seq_length=2048,
    dtype=None,
    load_in_4bit=True,
)

# Activer le mode inf√©rence
FastLanguageModel.for_inference(model)

print("‚úÖ Mod√®le charg√© !")
print("\n" + "="*50)
print("Test de Blablabruti")
print("="*50 + "\n")

# Questions de test
test_questions = [
    "Bonjour",
    "Quelle heure est-il ?",
    "Comment faire cuire des p√¢tes ?",
    "Qu'est-ce qu'une blockchain ?",
    "Quel est le sens de la vie ?",
]

for question in test_questions:
    print(f"üë§ Question: {question}")
    print("ü§ñ Blablabruti:")
    
    # Formater la prompt pour Mistral
    prompt = f"<|im_start|>user\n{question}<|im_end|>\n<|im_start|>assistant\n"
    inputs = tokenizer([prompt], return_tensors="pt").to("cuda")
    
    # G√©n√©rer la r√©ponse
    text_streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)
    _ = model.generate(
        **inputs,
        streamer=text_streamer,
        max_new_tokens=256,
        temperature=0.9,
        do_sample=True,
    )
    
    print("\n" + "-"*50 + "\n")

print("‚úÖ Tests termin√©s !")

